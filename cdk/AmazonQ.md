# BEACON プロジェクト - S3 -> SQS -> ETL -> Aurora パイプライン

## 概要

このプロジェクトでは、S3バケットにアップロードされたCSVファイルを自動的に処理し、Aurora Serverless v2 MySQLデータベースにETL（抽出・変換・ロード）するパイプラインを実装しています。

## アーキテクチャ

```
┌───────────┐     ┌───────────┐     ┌───────────┐     ┌───────────────┐
│           │     │           │     │           │     │               │
│    S3     │────>│    SQS    │────>│  Lambda   │────>│ Aurora MySQL  │
│           │     │           │     │           │     │               │
└───────────┘     └───────────┘     └───────────┘     └───────────────┘
```

1. CSVファイルがS3バケットの `csv/` プレフィックスにアップロードされる
2. S3イベント通知がSQSキューにメッセージを送信
3. Lambda関数がSQSキューからメッセージを受信し、CSVファイルを処理
4. 処理されたデータがAurora Serverless v2 MySQLデータベースに保存される

## 実装の詳細

### S3SqsEtlAurora コンストラクト

このCDKコンストラクトは、S3 -> SQS -> Lambda -> Aurora ETLパイプラインを実装します。

主な機能:
- 既存のS3バケットにCSVファイルアップロード時のイベント通知を設定
- SQSキューとデッドレターキューの作成
- Aurora Serverless v2 MySQLクラスターの作成
- ETL処理用のLambda関数の作成と設定
- 必要なIAMアクセス権限の設定

### ETL Lambda関数

Lambda関数は以下の処理を行います:
1. SQSメッセージからS3イベント情報を取得
2. S3からCSVファイルをダウンロード
3. CSVデータを解析
4. ファイル名に基づいてテーブルを自動作成（存在しない場合）
5. データをAurora MySQLにバッチ挿入

特徴:
- 動的なテーブル作成（CSVヘッダーに基づく）
- トランザクション処理によるデータ整合性の確保
- バッチ処理による効率的なデータ挿入
- エラーハンドリングとロギング

## 使用方法

1. CSVファイルをS3バケットの `csv/` プレフィックスにアップロード
   ```
   aws s3 cp data.csv s3://your-bucket-name/csv/
   ```

2. ファイル名がテーブル名になります（例: `users.csv` -> `users` テーブル）

3. CSVファイルの最初の行はヘッダー行として扱われ、テーブルのカラム名になります

## 注意事項

- 本番環境では、Aurora Serverless v2クラスターの削除保護を有効にしてください
- CSVファイルは適切なヘッダー行を含む必要があります
- 大量のデータを処理する場合は、Lambda関数のタイムアウト設定を調整してください
