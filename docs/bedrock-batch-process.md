# Approaches for Processing Large Document Volumes with Bedrock Claude

## Overview

When splitting PDF, Word, or Excel files into pages (each page treated as an “image” to process), there are three ways to call Amazon Bedrock’s Claude model for many pages in parallel:

1. **Lambda with internal parallel calls:** Use an AWS Lambda function (Node.js) that invokes Bedrock’s API for each page in parallel (e.g. via `Promise.all`).
2. **Step Functions Map + Lambdas:** Use an AWS Step Functions state machine with a Map state to fan out and invoke a Lambda for each page in parallel (each Lambda then calls Bedrock’s API for its page).
3. **Bedrock Batch Inference:** Use Bedrock’s batch job API to submit all pages in one request (as a batch job), letting Bedrock process them asynchronously and save results to S3.

Each approach has different **quotas/limits**, **cost** implications, and **speed** characteristics. The priorities are: **1) Avoid throttling**, **2) Cost effectiveness**, **3) Processing speed**. Below, we compare all three options on these factors, then provide a recommendation.

## Quotas and Throttling Limits

**Approach 1 – Lambda with parallel Bedrock API calls:** This relies on the Bedrock **on-demand API** for each page. Bedrock imposes per-model rate limits on on-demand calls – for example, Anthropic Claude models allow on the order of **tens to a few hundred requests per minute** depending on version and region (e.g. ~50 req/min in some regions, up to 250 req/min in us-west-2 for Claude 3.5 Sonnet) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet)) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet%20V2)). Exceeding these will trigger **throttling (HTTP 429)** errors. A single Lambda can issue many requests concurrently, but **it’s the developer’s responsibility to throttle the calls** to stay within Bedrock’s limits. Without control, calling Bedrock in parallel for dozens of pages _at once_ could easily exceed the model’s RPM quota and cause failures. Lambda itself can scale concurrency by running multiple instances, but if you process all pages within one Lambda invocation, the Lambda **concurrency limit** (default **1000 concurrent executions per region** ([Understanding Lambda function scaling - AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#:~:text=environments%20until%20you%20reach%20your,that%20your%20critical%20functions%20don%27t))) isn’t the bottleneck – the Bedrock API limit is. One Lambda invocation can run up to 15 minutes; if it needs to process hundreds of pages sequentially (or with limited parallelism) due to throttling, it could hit the timeout. In short, Approach 1 requires implementing your own concurrency limit or retry logic to **gracefully scale** without throttling (for example, capping concurrent Bedrock calls to ~50 per minute if that’s the model’s limit).

**Approach 2 – Step Functions Map + Lambda per page:** This approach also uses the Bedrock on-demand API under the hood, so **Bedrock’s per-minute rate limits are the same** as above. The difference is that Step Functions can orchestrate calls more systematically. A Step Functions **Map state** will spawn parallel Lambda executions for each page. By default, an _Inline_ Map can run up to **40 iterations concurrently** ([Using Map state in Inline mode in Step Functions workflows - AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/state-map-inline.html#:~:text=By%20default%2C%20,the%20parent%20workflow%27s%20execution%20history)). With the newer _Distributed_ Map mode, you can go far beyond (up to 10,000 parallel executions if unconstrained) ([Using Map state in Distributed mode for large-scale parallel workloads in Step Functions - AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/state-map-distributed.html#:~:text=Specifies%20the%20number%20of%20child,that%20evaluates%20to%20an%20integer)). **However, you should configure a `MaxConcurrency`** in the Map state to avoid overrunning Bedrock’s rate limit. For example, if Claude’s limit is ~50 calls/min, you might set a concurrency of ~50 (or slightly less) to stay safe. Step Functions will then ensure only that many Lambdas run in parallel, queuing the rest – this provides **built-in throttling control**. The Lambda concurrency quota (default 1000 concurrent Lambdas per region) still applies ([Understanding Lambda function scaling - AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#:~:text=environments%20until%20you%20reach%20your,that%20your%20critical%20functions%20don%27t)), but this is usually higher than the Bedrock limit, so Bedrock is the gating factor. In practice, Approach 2 can be tuned to avoid API throttling by adjusting the Map’s parallelism. Each Lambda invocation would typically handle one page (one Bedrock call), so the **15-minute Lambda limit is not a concern** (each Lambda runs briefly). If a Bedrock call does get throttled or fails, you can handle retries per Lambda or let the state machine catch errors and retry the state – giving fine-grained control.

**Approach 3 – Bedrock Batch Inference:** This approach uses Bedrock’s batch job API, which is **asynchronous**. You submit all pages in one job (as a JSONL file in S3), and Bedrock processes them internally. This avoids the need for the client to send one API request per page. **Throttling is essentially handled by Bedrock’s managed service** – you won’t receive 429 errors as long as the job is accepted. There are different quotas to consider here: Batch jobs require a **minimum of 100 records** per job ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Minimum%20number%20of%20records%20per,5%20Sonnet%20Each)) (for Claude models), and support up to **50,000 records in one job** ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Records%20per%20batch%20inference%20job,5%20Sonnet)). (A “record” in this context is one page’s input.) For our scale (1–1000 pages), the **100-record minimum** means very small documents (under 100 pages) can’t be run as a single batch job unless combined with others. Batch input files also have size limits (e.g. max 1 GB per JSONL file, and up to 5 GB total per job). Another quota is at the job level: you can have up to **20 batch jobs in progress per model** ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Sum%20of%20in,5%20Sonnet)) (not usually an issue unless you submit many large docs at once). **Concurrent processing** within a batch job is handled by Bedrock – the service will parallelize as it sees fit, within its system constraints. Crucially, you as the user do not need to worry about per-minute call limits; Bedrock will queue and execute the requests without throttling your API calls. This makes Approach 3 very **graceful to scale** – even if you submit 1000 pages, the job will process all of them without API rate errors (the trade-off being it runs to completion asynchronously).

## Cost Efficiency Comparison

**Approach 1 – Lambda with parallel calls:** The cost drivers here are **Bedrock Claude inference costs** and **Lambda execution costs**. Bedrock on-demand pricing applies for each page’s inference. For example, Claude 3.5 Sonnet is about **$0.003 per 1K input tokens and $0.015 per 1K output tokens** on-demand ([Build Generative AI Applications with Foundation Models – Amazon Bedrock Pricing – AWS](https://aws.amazon.com/bedrock/pricing/#:~:text=Claude%203.5%20Sonnet)). This cost will be the same for Approach 1 and 2, since both use on-demand calls. Lambda’s cost (GB-seconds of compute) is typically very small relative to model inference cost – even if one Lambda handles 1000 pages, it might run for e.g. a few hundred seconds total; with memory allocation of a few hundred MB, that might be only a few cents. In practice, **model usage charges dominate the cost**. There is no additional cost for parallelism inside the Lambda beyond the runtime. So Approach 1’s main cost disadvantage is simply that it pays standard Bedrock rates for potentially a large number of pages. Each Bedrock call is metered separately in on-demand mode.

**Approach 2 – Step Functions + Lambdas:** This approach incurs the **same Bedrock on-demand token costs** as Approach 1 for the Claude model. In addition, there are small costs for Step Functions and Lambda overhead. Step Functions Standard Workflows charge per state transition (roughly $0.025 per 1,000 transitions, though varies by region). If you process 1000 pages, that’s ~1000 iterations plus some overhead states – on the order of a few thousand transitions at most (i.e. only pennies in cost). Each page runs in a separate Lambda invocation; the aggregate Lambda compute time might be similar to Approach 1’s single Lambda (the total CPU/wall-clock time needed to invoke 1000 model calls). There is some extra overhead for initialization in each function, but Lambda invocations of a second or two each are still very cheap (micro-dollars each). **Overall, the additional cost of Step Functions orchestration is negligible compared to Claude’s inference cost** for large documents. You do pay per invocation rather than one invocation, but Lambda’s cost is per duration, so 1000 short Lambdas costing, say, 2 ms billed each might add up to a similar few cents as one long Lambda – the difference isn’t significant at scale. In summary, Approach 2 might be _slightly_ more expensive than Approach 1 in overhead (because of state machine and many Lambda starts), but this difference is minimal. Both Approach 1 and 2 use on-demand Bedrock pricing, so they are **significantly more expensive per token** than Approach 3.

**Approach 3 – Bedrock Batch Inference:** Batch mode offers a **substantial cost advantage**: Amazon Bedrock charges **50% less for batch inference** compared to on-demand ([Build Generative AI Applications with Foundation Models – Amazon Bedrock Pricing – AWS](https://aws.amazon.com/bedrock/pricing/#:~:text=With%20Batch%20mode%2C%20you%20can,refer%20to%20model%20list%20here)). The same Claude model invocation, when done via a batch job, costs half the price in terms of $/token. This means if processing hundreds or thousands of pages, the savings are significant. For example, if on-demand inference for 1000 pages would cost $10, doing it via batch might cost about $5 (half the price) for the same tokens. There is **no Lambda cost** for running the model in batch (unless you use Lambda to prepare or submit the job, which is a negligible one-time call). The batch job writing outputs to S3 might incur a tiny S3 PUT and storage cost, but that’s essentially negligible. So in terms of cost efficiency, Approach 3 is the best – **especially at scale, batch inference can cut your model usage bill in half**. The only scenario where batch might not save cost is if you frequently have documents too small to meet the 100-record minimum – in those cases, you might be forced to use on-demand (and pay full price) unless you bundle multiple docs into one batch. But for large volumes, batch is clearly more economical.

Below is a summary table of quotas/limits and cost factors for each approach:

| **Factor**              | **(1) Lambda + Promise.all** <br>_Parallel API calls in one Lambda_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **(2) Step Functions Map** <br>_Parallel Lambdas for each page_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | **(3) Bedrock Batch Inference** <br>_Asynchronous batch job_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Quotas & Throttling** | • **Bedrock API**: ~50–250 calls/minute limit for Claude (region/model dependent) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet)) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet%20V2)) – must throttle within code to avoid 429 errors.<br>• **Lambda**: 1,000 concurrent executions account limit ([Understanding Lambda function scaling - AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#:~:text=environments%20until%20you%20reach%20your,that%20your%20critical%20functions%20don%27t)) (if multiple Lambdas); one Lambda max 15 min runtime.<br>• No built-in throttle control – needs custom concurrency limiting or retries.                                                                        | • **Step Functions**: Inline Map max 40 concurrent iterations ([Using Map state in Inline mode in Step Functions workflows - AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/state-map-inline.html#:~:text=By%20default%2C%20,the%20parent%20workflow%27s%20execution%20history)); Distributed Map can spawn up to 10,000 in parallel if not limited ([Using Map state in Distributed mode for large-scale parallel workloads in Step Functions - AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/state-map-distributed.html#:~:text=Specifies%20the%20number%20of%20child,that%20evaluates%20to%20an%20integer)).<br>• **MaxConcurrency** can be set (e.g. ~50) to respect Claude’s rate limit and prevent API throttling.<br>• **Lambda**: 1,000 concurrent Lambdas limit ([Understanding Lambda function scaling - AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#:~:text=environments%20until%20you%20reach%20your,that%20your%20critical%20functions%20don%27t)) – Map state will queue beyond this. Each Lambda handles one page, so 15-min runtime limit is not an issue.<br>• Robust to scale; failures can be handled per page. | • **Batch jobs**: Min **100 records** per job ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Minimum%20number%20of%20records%20per,5%20Sonnet%20Each)), up to 50k records ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Records%20per%20batch%20inference%20job,5%20Sonnet)); input file ≤1 GB, total job ≤5 GB (Claude).<br>• Max 20 concurrent jobs per model ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=Sum%20of%20in,5%20Sonnet)) (more can queue).<br>• **No client-side rate limits** – one API call submits the whole batch. Bedrock handles throttling internally, so virtually no 429 errors for the user. |
| **Cost**                | • **Bedrock on-demand pricing** (Claude standard rate per input/output token). No volume discount.<br>• **Lambda** compute cost for one long-running function (processing many requests) – usually minimal relative to FM usage.<br>• Overall cost = ∑(page token costs) @ on-demand rate + a tiny Lambda cost.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | • **Bedrock on-demand pricing** (same token cost as Approach 1).<br>• **Step Functions** cost: ~$0.025 per 1k state transitions (pennies for hundreds of pages).<br>• **Lambda** cost for many short invocations – slightly higher overhead, but still negligible compared to model cost.<br>• Overall cost ≈ Approach 1 (on-demand FM costs dominate), plus a small additional orchestrator cost.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | • **Bedrock batch pricing** – **50% lower cost per token** than on-demand ([Build Generative AI Applications with Foundation Models – Amazon Bedrock Pricing – AWS](https://aws.amazon.com/bedrock/pricing/#:~:text=With%20Batch%20mode%2C%20you%20can,refer%20to%20model%20list%20here)) (significant savings at scale).<br>• No Lambda or Step Functions needed for inference (just one job submission call).<br>• Minor costs for S3 storage of input/output.<br>• Overall, **most cost-effective** for large numbers of pages (half the FM cost of approaches 1 & 2 for the same workload).                                                                                                                                                                                                                                                                                   |
| **Processing Speed**    | • **Concurrency**: Can fire many calls in parallel, but **effective throughput is capped by Bedrock’s limit**. In practice, you should limit to ~ model’s max calls/min (e.g. 50–250/min) to avoid throttling. ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet)) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=On,5%20Sonnet%20V2)) <br>• A single Lambda can issue requests asynchronously (non-blocking), so it can keep many in flight. If concurrency is set to the allowed max, total time ≈ pages ÷ (allowed RPS). E.g. at 50 req/min, 1000 pages ≈ 20 minutes; at 250/min, 1000 pages ≈ 4 minutes.<br>• Low overhead per request (no extra process launch per page). However, if throttling is hit and retries/back-off occur, that increases latency. | • **Concurrency**: Also ultimately bounded by Bedrock’s limit (you will configure MaxConcurrency accordingly). You can saturate the allowed throughput similarly (e.g. 50–250/min). The Step Functions will ensure no more than that run concurrently, making full use of the quota without exceeding it.<br>• Overhead: Each page Lambda has a cold start (tens of ms) and invokes Bedrock. The state machine adds slight latency. But since all Lambdas run in parallel (up to the limit), overall throughput is comparable to Approach 1’s best-case, and coordination overhead is minor. <br>• Scaling: For very large numbers of pages (thousands+), Step Functions can queue and execute without hitting a single Lambda timeout. It can efficiently handle batches that one Lambda alone might struggle with.                                                                                                                                                                                                                                                                                                                                                                                                         | • **Batch**: Not instantaneous – the job runs asynchronously. Bedrock may process many pages in parallel internally, but you have **no direct control over concurrency or exact timing**. For 1000 pages, it might complete in a few minutes or longer depending on load (Bedrock aims to process large batches efficiently, but there’s no real-time guarantee). ([Enhance call center efficiency using batch inference for transcript summarization with Amazon Bedrock                                                                                                                                                                                                                                                                                                                                                                                                         | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/enhance-call-center-efficiency-using-batch-inference-for-transcript-summarization-with-amazon-bedrock/#:~:text=Solution%20overview))<br>• **Latency per page** is higher: you have to upload the input, start the job, then wait for the entire batch output. There’s some fixed overhead (job startup and wind-down). If you need a single page result immediately, this is not suitable. But for offline large-volume processing, the total throughput is reasonable. (Batch jobs can leverage cross-region capacity and optimized throughput behind the scenes, just at a lower priority than on-demand.) <br>• In summary, Approach 3 trades some speed for reliability and cost: it ensures all pages are processed without manual throttling, but you **wait for the batch to finish** rather than getting each result in real-time. |

**Interpretation:** Approaches 1 and 2 can achieve similar throughput, but both must be carefully managed to avoid Bedrock’s on-demand rate limits. Approach 2 provides more structured control over concurrency (via Step Functions) and can scale to larger workloads without hitting Lambda runtime limits. Approach 3 operates under a different model – it won’t return results as quickly as the others in a real-time sense, but it **eliminates throttling risk by design** and **slashes the cost per page by 50%**, which is a huge advantage for large volumes. Speed is the lowest priority here, so the slower batch job approach is acceptable as long as it scales smoothly.

## Processing Speed Considerations

While speed is the third priority, it’s still worth noting how each approach might perform:

- **Lambda with Promise.all:** If unconstrained, this could try to process all pages at once, which would be very fast per page, but in reality Bedrock would throttle that. When adding proper throttling (e.g. a limit on concurrent promises or a rate limiter), the speed becomes “as fast as allowed by Bedrock.” This is typically the limiting factor. For instance, running 100 calls in parallel when the limit is 50/min will just cause half to fail and retry, so you gain nothing. The optimal scenario is to match Bedrock’s throughput. With a well-tuned concurrency, Approach 1 can complete 1000 pages in roughly the minimum time Bedrock permits (a few minutes up to tens of minutes, depending on the model’s quotas). There’s a slight chance one Lambda doing all work might be marginally faster or more efficient for small tasks (no overhead of starting new processes), but for large tasks it could actually slow down if it starts hitting runtime limits or memory constraints. Also, if one page’s processing is slow (e.g. a particularly long response), it could hold up that Lambda’s execution of others if not truly parallel.

- **Step Functions parallel Lambdas:** This approach might add a bit of overhead per page (each Lambda has to start up), but it allows very **wide parallelism** if the limits are high. If Claude’s model limit were higher or if you had provisioned throughput, Step Functions could spin up hundreds of Lambdas to use that capacity. In our scenario, since we want to avoid throttling, we’d intentionally not exceed the safe concurrency. So speed will end up similar to Approach 1’s controlled speed. One benefit is that results could be collected as they come (each Lambda could write to a result store when done), whereas the single Lambda approach might accumulate results and return them all at once. However, in both cases we’re likely doing offline processing, so this isn’t a big differentiator. Step Functions shines if we had an extremely large number of pages (say 10,000+): it could batch them in groups and even run iterations in **batches** (using the Distributed Map’s batching feature) so that you process N pages per Lambda if needed to reduce overhead. But for up to 1000 pages, we probably don’t need that level of optimization.

- **Bedrock Batch:** The batch job will not begin instantaneously like a function invocation; there might be a queue or provisioning delay (especially if you submit many jobs or a very large job). Amazon Bedrock doesn’t publish an explicit SLA for batch job duration, but anecdotal usage suggests it can process on the order of thousands of requests in minutes (the Anthropic documentation for their similar API mentions up to 10k queries processed in under 24 hours, and Bedrock’s own blog suggests batch inference is used for large-scale tasks where real-time latency isn’t critical ([Enhance call center efficiency using batch inference for transcript summarization with Amazon Bedrock | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/enhance-call-center-efficiency-using-batch-inference-for-transcript-summarization-with-amazon-bedrock/#:~:text=Batch%20inference%20presents%20itself%20as,are%20not%20always%20a%20requirement)) ([Enhance call center efficiency using batch inference for transcript summarization with Amazon Bedrock | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/enhance-call-center-efficiency-using-batch-inference-for-transcript-summarization-with-amazon-bedrock/#:~:text=compared%20to%20real,are%20not%20always%20a%20requirement))). In practice, 1000 pages is a moderate batch; we can reasonably expect the job to complete in perhaps a few minutes to maybe tens of minutes worst-case. Since we have _no hard time limit_ and speed is least important, this is acceptable. The key is that **all pages will be processed in one go without intervention**, and you won’t have to babysit any throttling logic. If we needed to accelerate processing beyond what on-demand allows (for example, truly concurrently beyond the 50–250/min limit), batch might not magically be faster unless Bedrock internally distributes to multiple regions or uses idle capacity. But given that on-demand itself could be scaled across regions (Bedrock also supports cross-region inference to increase throughput, though Claude’s cross-region limits are similar in this case), the difference is likely that batch is handled at a slightly “slower but cheaper” rate. It’s a trade-off of cost vs. latency.

## Pros and Cons Summary

**Approach 1 – Lambda with internal parallel calls:**

- **Pros:** Simple to implement within a single function; no need for managing an external workflow. Good for relatively small numbers of pages or interactive use (e.g. if you had <=100 pages, you can just invoke one Lambda and get results quickly). No Step Functions overhead. All results aggregated in one place (the Lambda’s memory) for easy collation.

- **Cons:** Risk of **API throttling** if concurrency is not carefully managed – you must implement your own limits or retries. Not as scalable for very large jobs (one Lambda handling 1000+ calls could hit the 15 min limit or exhaust memory if responses are large). Harder to pinpoint failures (if one call fails, you have to catch it and handle it within the function). No cost savings on the model usage. In short, it’s a bit **fragile at high scale** – requires careful tuning to avoid overload.

**Approach 2 – Step Functions + per-page Lambda:**

- **Pros:** Highly **scalable and resilient**. The Step Functions Map can easily fan out to hundreds of Lambdas, and with `MaxConcurrency` you can enforce a safe throttle, achieving **graceful scaling** without hitting Bedrock’s limits. Each page is isolated – one failure doesn’t crash the whole batch; you can even configure tolerances or retries per item. It naturally handles large workloads by distributing across many Lambdas (no single bottleneck). Also easier to **monitor progress** (e.g. you can see how many state tasks succeeded/failed). No concern about Lambda timeout for the overall job.

- **Cons:** Slightly **more complex architecture** – requires a Step Functions state machine and Lambda function code for the page processing. There is a bit of orchestration overhead (could be overkill for very small jobs, where a single Lambda would suffice). You still pay full on-demand price for Claude. While Step Functions shields you from manual throttling, you still need to know the limits and set the concurrency appropriately (it doesn’t automatically know the model’s quota). Also, if your workflow is asynchronous anyway, this approach doesn’t save on cost like batch does – it mainly helps with structure and reliability.

**Approach 3 – Bedrock Batch Inference:**

- **Pros:** **No throttling concerns at all** – Amazon Bedrock manages the parallelization and will ensure the model is invoked as capacity allows. You can throw a very large number of pages into a job and let it run to completion. **Most cost-effective** – 50% lower cost per token ([Build Generative AI Applications with Foundation Models – Amazon Bedrock Pricing – AWS](https://aws.amazon.com/bedrock/pricing/#:~:text=With%20Batch%20mode%2C%20you%20can,refer%20to%20model%20list%20here)) is a huge advantage for large volumes. Simplified workflow for big batches: just prepare the input file and submit one API call (or via console). Good for offline processing where you don’t need results immediately. Also, batch jobs are retriable or can be broken up if needed, and Bedrock will output all results to an S3 file which is convenient for subsequent processing. According to AWS, this feature is specifically meant to “process large volumes of data” in a scalable way ([Enhance call center efficiency using batch inference for transcript summarization with Amazon Bedrock | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/enhance-call-center-efficiency-using-batch-inference-for-transcript-summarization-with-amazon-bedrock/#:~:text=Solution%20overview)) – it’s the intended solution for high-volume, non-real-time jobs.

- **Cons:** **Minimum batch size of 100** pages – not suitable for smaller documents unless you combine multiple docs into one batch. There’s an inherent **delay** in getting results (jobs may take a few minutes or more to complete). This approach is not ideal if you need an on-demand response or if you have an interactive application (since you’d have to poll for results). Setting up the JSONL file and handling S3 outputs introduces a bit of overhead in your pipeline (as opposed to getting an immediate API response in Lambda). Also, debugging a batch job is less granular – if one record fails, the job might fail or complete with an error report; you’d then have to possibly re-run just those failed records. However, these downsides are manageable for an offline, large-scale processing scenario.

## Recommendation

Given the priorities, the **best overall approach is to use Amazon Bedrock’s Batch Inference API (Approach 3) for large volumes**, as long as the workload is suitable for batch processing. This approach **completely avoids throttling issues** by design – you won’t run into Bedrock’s request-per-minute limits, because the service handles queuing and scaling internally. It also offers **far better cost efficiency** at scale (half the cost per token of the on-demand calls) ([Build Generative AI Applications with Foundation Models – Amazon Bedrock Pricing – AWS](https://aws.amazon.com/bedrock/pricing/#:~:text=With%20Batch%20mode%2C%20you%20can,refer%20to%20model%20list%20here)), which will make a big difference when processing hundreds or thousands of pages. Although batch processing may not return results as quickly as the on-demand calls, speed is the lowest priority and the difference is acceptable for most offline document processing use cases. Bedrock’s batch feature is specifically built for “processing large volumes of data” reliably ([Enhance call center efficiency using batch inference for transcript summarization with Amazon Bedrock | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/enhance-call-center-efficiency-using-batch-inference-for-transcript-summarization-with-amazon-bedrock/#:~:text=Solution%20overview)), which matches this scenario.

That said, we should consider a **hybrid strategy** to cover all page counts from 1 to 1000, because Bedrock batch jobs require at least 100 records. For **documents with <100 pages**, you have two options: **(a)** accumulate multiple small documents into one batch job (if you can afford to wait and group them) until you meet the minimum, or **(b)** fall back to on-demand processing for those cases. If real-time processing for small files is needed, Approaches 1 or 2 can be used on a case-by-case basis. Among those, **Step Functions (Approach 2)** is safer for scaling out if there’s any risk of bursts. For example, if your typical workload is to process one document at a time, and it’s usually under 100 pages, you might simply call Bedrock’s API directly (Lambda or otherwise). But if you ever need to handle many such documents concurrently (or one very large document section-by-section), having the Step Functions Map implementation ready would allow you to do so without throttling – you’d set `MaxConcurrency` to a reasonable level (based on Claude’s rate limit, e.g. 50) so it **scales gracefully**. Retries for throttling errors can be implemented in the Lambda or via Step Functions error handling as a backstop.

In summary, **use Bedrock Batch Inference for large batches** to maximize throughput and cost savings, and consider using the on-demand API (via a controlled Step Functions workflow) for smaller jobs or cases where you can’t meet the batch minimum. This hybrid approach gives you the best of both: you’ll avoid throttling by using batch whenever possible, save money on big jobs, and still handle small real-time tasks when needed. Key tuning suggestions include: if using on-demand calls in parallel, set an appropriate concurrency limit (don’t exceed ~50 calls per second for Claude, or whatever the per-minute limit translates to) to prevent 429 errors ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=us)). Implement exponential backoff retries for any occasional throttling responses. Also, choose the region wisely – for instance, us-west-2 has higher Claude throughput quotas ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=us)) ([Amazon Bedrock endpoints and quotas - AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#:~:text=us)) than some other regions, which can help avoid throttling if you’re using on-demand.

Overall, **Approach 3 (Batch)** is recommended as the primary solution for its scalability and cost benefits, with Approaches 1/2 as supporting solutions for cases where batch is not applicable. This will ensure you can process 1–1000 pages **efficiently, cost-effectively, and without hitting throttling limits**.
